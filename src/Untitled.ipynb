{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dece4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sys\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "from copy import deepcopy as dp\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n",
    "    ss_1_dic = {'zsco':StandardScaler(),\n",
    "                'mima':MinMaxScaler(),\n",
    "                'maxb':MaxAbsScaler(), \n",
    "                'robu':RobustScaler(),\n",
    "                'norm':Normalizer(), \n",
    "                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n",
    "                'powe':PowerTransformer()}\n",
    "    ss_1 = ss_1_dic[sc_name]\n",
    "    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    if saveM == False:\n",
    "        return(df_2)\n",
    "    else:\n",
    "        return(df_2,ss_1)\n",
    "\n",
    "def norm_tra(df_1,ss_x):\n",
    "    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    return(df_2)\n",
    "\n",
    "def g_table(list1):\n",
    "    table_dic = {}\n",
    "    for i in list1:\n",
    "        if i not in table_dic.keys():\n",
    "            table_dic[i] = 1\n",
    "        else:\n",
    "            table_dic[i] += 1\n",
    "    return(table_dic)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        cha_1 = 256 // 8\n",
    "        cha_2 = 512 // 8\n",
    "        cha_3 = 512 // 8\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(hidden_size/cha_1/2)\n",
    "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "        self.dropout_c1 = nn.Dropout(0.1)\n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2 = nn.Dropout(0.1)\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_1 = nn.Dropout(0.3)\n",
    "        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_2 = nn.Dropout(0.2)\n",
    "        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        x = x.reshape(x.shape[0],self.cha_1,\n",
    "                          self.cha_1_reshape)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = self.dropout_c1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c2_1(x)\n",
    "        x = self.dropout_c2_1(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "\n",
    "        x = self.batch_norm_c2_2(x)\n",
    "        x = self.dropout_c2_2(x)\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x =  x * x_s\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
    "#tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
    "#tar_weight0_min = dp(np.min(tar_weight0))\n",
    "#tar_weight = tar_weight0_min/tar_weight0\n",
    "#pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
    "                                                      pos_weight = None)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1649017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features.iloc[idx, :].values, dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets.iloc[idx, :].values, dtype=torch.float)            \n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features.iloc[idx, :].values, dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f312613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "\n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # * imputing: the missing values are replaced in all input columns following a simple constant strategy (fill value is −1);\n",
    "    # * quantization: each input column is discretized into quantile bins, and the number of these bins is detected\n",
    "    # automatically; after that the bin identifier can be considered quantized numerical value of the original feature;\n",
    "    # * standardization: each quantized column is standardized by removing the mean and scaling to unit variance;\n",
    "    # * decorrelation: all possible linear correlations are removed from the feature vectors discretized by above-mentioned way;\n",
    "    # the decorrelation is implemented using PCA\n",
    "    df.fillna(-1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "input_dir = '../data/'\n",
    "\n",
    "train_df = pd.read_csv(input_dir+'shifts_canonical_train.csv')\n",
    "x_train = train_df[train_df.columns.drop(['climate'] + list(train_df.filter(regex='fact_')))].astype(np.float32)\n",
    "y_train = train_df['fact_temperature'].astype(np.float32).to_frame()\n",
    "\n",
    "\n",
    "val_df_inDom = pd.read_csv(input_dir+'shifts_canonical_dev_in.csv')\n",
    "x_valid_inDom = val_df_inDom[val_df_inDom.columns.drop(['climate'] + list(val_df_inDom.filter(regex='fact_')))].astype(np.float32)\n",
    "y_valid_inDom = val_df_inDom['fact_temperature'].to_frame()\n",
    "\n",
    "val_df_outDom = pd.read_csv(input_dir+'shifts_canonical_dev_out.csv')\n",
    "x_valid_outDom = val_df_outDom[val_df_outDom.columns.drop(['climate'] + list(val_df_outDom.filter(regex='fact_')))].astype(np.float32)\n",
    "y_valid_outDom = val_df_outDom['fact_temperature'].to_frame()\n",
    "\n",
    "\n",
    "test_df_inDom = pd.read_csv(input_dir+'shifts_canonical_eval_in.csv')\n",
    "x_test_inDom = test_df_inDom[test_df_inDom.columns.drop(['climate'] + list(test_df_inDom.filter(regex='fact_')))].astype(np.float32)\n",
    "y_test_inDom = test_df_inDom['fact_temperature'].to_frame()\n",
    "\n",
    "test_df_outDom = pd.read_csv(input_dir+'shifts_canonical_eval_out.csv')\n",
    "x_test_outDom = test_df_outDom[test_df_outDom.columns.drop(['climate'] + list(test_df_outDom.filter(regex='fact_')))].astype(np.float32)\n",
    "y_test_outDom = test_df_outDom['fact_temperature'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "seed = 42\n",
    "\n",
    "n_comp1 = 50\n",
    "n_comp2 = 15\n",
    "\n",
    "feature_cols= x_train.columns.values.tolist()\n",
    "target_cols = ['fact_temperature']\n",
    "num_features=len(feature_cols) # + n_comp1 + n_comp2\n",
    "num_targets=len(target_cols)\n",
    "#num_targets_0=len(target_nonsc_cols2)\n",
    "hidden_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea774e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocess(x_train)\n",
    "# y_train\n",
    "\n",
    "x_valid_inDom = preprocess(x_valid_inDom)\n",
    "# y_valid_inDom\n",
    "\n",
    "x_valid_outDom = preprocess(x_valid_outDom)\n",
    "# y_valid_outDom\n",
    "\n",
    "# x_test_inDom = x_test_inDom\n",
    "# y_test_inDom\n",
    "\n",
    "# x_test_outDom = x_test_outDom\n",
    "# y_test_outDom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(x_train, y_train)\n",
    "valid_dataset_inDom = TrainDataset(x_valid_inDom, y_valid_inDom)\n",
    "valid_dataset_outDom = TrainDataset(x_valid_outDom, y_valid_outDom)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader_inDom = torch.utils.data.DataLoader(valid_dataset_inDom, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validloader_outDom = torch.utils.data.DataLoader(valid_dataset_outDom, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "            num_features=num_features,\n",
    "            num_targets=num_targets,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n",
    "loss_va_inDom = nn.BCEWithLogitsLoss()\n",
    "loss_va_outDom = nn.BCEWithLogitsLoss()\n",
    "\n",
    "early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "early_step = 0\n",
    "\n",
    "oof = np.zeros(len(target_cols))\n",
    "best_loss = np.inf\n",
    "\n",
    "mod_name = f\"FOLD_mod11_{seed}.pth\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "    valid_loss_inDom, valid_preds_inDom = valid_fn(model, loss_va_inDom, validloader_inDom, DEVICE)\n",
    "    valid_loss_outDom, valid_preds_outDom = valid_fn(model, loss_va_outDom, validloader_outDom, DEVICE)\n",
    "    print(f\"SEED: {seed}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss_inDom: {valid_loss_inDom}, valid_loss_outDom: {valid_loss_outDom}\")\n",
    "\n",
    "    if valid_loss_outDom < best_loss:\n",
    "\n",
    "        best_loss = valid_loss_outDom\n",
    "        oof = valid_preds_outDom\n",
    "        torch.save(model.state_dict(), mod_name)\n",
    "\n",
    "    elif(EARLY_STOP == True):\n",
    "\n",
    "        early_step += 1\n",
    "        if (early_step >= early_stopping_steps):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639df651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fea95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pdiot] *",
   "language": "python",
   "name": "conda-env-pdiot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
